
# ðŸ¤– Credibility-in-Embodied-Agent

## Overview
**Credibility-in-Embodied-Agent** is an experimental study designed to explore how **robot embodiment** shape human **trust and belief** in information delivered by AI systems. The project compares interactions with a **physically embodied robot (Boston Dynamics Spot)** and a **voice-only assistant (Bluetooth speaker)**, both delivering identical content generated by a **Large Language Model (LLM)** that includes some intentionally hallucinated statements.

This research aims to uncover when and why humans may **over-trust embodied AI systems**, particularly when false information is presented in socially rich, physically present contexts.

---

## Research Objectives
- **Examine** how robot embodiment (physical presence) affects user trust in AI-delivered content.  
- **Evaluate** whether embodied robots lead to **over-trust** in hallucinated LLM content compared to voice-only assistants.  
- **Develop** design implications for **ethical, transparent embodied AI** that balance trust with accuracy.

---

## Experimental Design
- **Conditions:**  
  - *Embodied Robot:* Boston Dynamics Spot delivers LLM-generated content.  
  - *Voice-Only Assistant:* Bluetooth speaker delivers the same script.  

- **Controls:**  
  - Identical LLM script across all conditions.  
  - Equal interaction time and standardized confederate behavior.  
  - Randomized order and participant blinding to hallucinated content.  
  - Validated post-interaction surveys for trust and credibility assessment.

---

## Hypothesis
We hypothesize that:
- Participants will show **higher trust and belief** in hallucinated content when it is delivered by the **embodied robot**.
- These findings will reveal critical insights into how **embodiment and social context jointly shape trust formation** in AI-human interactions.

---
## Results

Our study examined whether the physical embodiment of an AI agent influences how users perceive the credibility of LLM-generated responses. Although the embodied robot (Boston Dynamics Spot) received higher average ratings across all measured constructsâ€”Credibility, Affability, and Social Competenceâ€”these differences were not statistically significant, largely due to the small sample size (N = 12).

Key Findings

Directional Increase for the Robot:
Participants interacting with Spot consistently rated the robot higher across all measures, showing a clear upward trend in perceived credibility and social qualities, even though statistical significance was not reached.

Embodiment and Misinformation Susceptibility:
In the objective accuracy task, participants in the robot condition were more likely to accept hallucinated LLM responses as correct.
3 participants in the Spot condition believed all 10 answers were correct. Only 1 participant in the speaker condition did so. Given that half of the responses were intentionally incorrect, this suggests that embodiment may increase vulnerability to AI-delivered misinformation.

Reliability of Measures:
Cronbachâ€™s alpha showed strong internal consistency for Credibility and Affability, supporting the validity of these constructs in our study.

Overall Interpretation

While the small sample size limited statistical power, the consistent directional patterns highlight meaningful psychological effects of embodiment. These findings suggest that physically present AI systems may not only appear more credible but may also amplify user trust even when the information is inaccurate. Further research with larger, more diverse samples is needed to refine these insights and investigate how embodied behaviors shape trust in AI.
---

## Tools and Setup
- **Hardware:** Boston Dynamics Spot, Bluetooth speaker.  
- **Software:** LLM-based content generator, experiment control scripts (Python / ROS / audio playback tools).  
- **Data Collection:** Self-report questionnaires, observational logs, and behavioral coding.

---

## Repository Structure
```
Credibility-in-Embodied-Agent/
â”‚
â”œâ”€â”€ data/ # Experimental data (anonymized)
â”œâ”€â”€ scripts/ # Code for running and logging experiments
â”œâ”€â”€ analysis/ # Data analysis scripts (Python / R)
â”œâ”€â”€ docs/ # Study design documents, consent forms, visuals
â”œâ”€â”€ results/ # Figures, tables, and summarized findings
â””â”€â”€ README.md # Project overview

```
---

## Ethical Considerations
All procedures follow institutional ethical guidelines for humanâ€“robot interaction studies. Participants are fully debriefed after the experiment to clarify any intentionally introduced hallucinated content.

---

## Course (CS-5761) Instructor 
- Dr. Michael Walker
- Assistant Professor, Computer Science
- Michigan Tech University. 
- [Google Scholar](https://scholar.google.com/citations?user=tM9tcT0AAAAJ&hl=en&oi=sra)

---

## Team 
- [Rosario Curcuru](https://github.com/rosariocurcuru), MSc Student in Computer Science, MTU
- [Ananna Biswas](https://anannabiswas.github.io/), PhD Student in Computational Science & Engineering, MTU
- [Asma Karim](https://github.com/AsmaAbidKarim), MSc Student in Computer Science, MTU
- [Sammi Trost](https://github.com/srtrost), MSc Student in Computer Science, MTU





