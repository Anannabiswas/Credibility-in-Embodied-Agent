
# ðŸ¤– Credibility-in-Embodied-Agent

## Overview
**Credibility-in-Embodied-Agent** is an experimental study designed to explore how **robot embodiment** shape human **trust and belief** in information delivered by AI systems. The project compares interactions with a **physically embodied robot (Boston Dynamics Spot)** and a **voice-only assistant (Bluetooth speaker)**, both delivering identical content generated by a **Large Language Model (LLM)** that includes some intentionally hallucinated statements.

This research aims to uncover when and why humans may **over-trust embodied AI systems**, particularly when false information is presented in socially rich, physically present contexts.

---

## Research Objectives
- **Examine** how robot embodiment (physical presence) affects user trust in AI-delivered content.  
- **Evaluate** whether embodied robots lead to **over-trust** in hallucinated LLM content compared to voice-only assistants.  
- **Develop** design implications for **ethical, transparent embodied AI** that balance trust with accuracy.

---

## Experimental Design
- **Conditions:**  
  - *Embodied Robot:* Boston Dynamics Spot delivers LLM-generated content.  
  - *Voice-Only Assistant:* Bluetooth speaker delivers the same script.  

- **Controls:**  
  - Identical LLM script across all conditions.  
  - Equal interaction time and standardized confederate behavior.  
  - Randomized order and participant blinding to hallucinated content.  
  - Validated post-interaction surveys for trust and credibility assessment.

---

## Hypothesis
We hypothesize that:
- Participants will show **higher trust and belief** in hallucinated content when it is delivered by the **embodied robot**.
- These findings will reveal critical insights into how **embodiment and social context jointly shape trust formation** in AI-human interactions.

---
## Results

Our study examined whether physical embodiment influences how users perceive the credibility of LLM-generated responses. Although the embodied robot (Boston Dynamics Spot) received higher average ratings across Credibility, Affability, and Social Competence, these differences were not statistically significant due to the small sample size (N = 12).

### Key Findings

- **Higher perceived credibility for the robot:**  
  Spot showed consistently higher mean ratings across all subjective measures, though not significant.

- **Greater acceptance of hallucinated content:**  
  Participants in the robot condition were more likely to believe all responses were correct. **3 participants** in the Spot condition vs. **1 participant** in the speaker condition believed all 10 answers were accurate, even though half   were intentionally incorrect.

- **Measure reliability:**  
  Credibility and Affability demonstrated strong internal consistency. Social Competence showed lower reliability and may require refined items.

### Summary
While embodiment did not yield statistically significant effects, the overall trends suggest that physically present AI agents may subtly increase perceived credibility and susceptibility to AI-generated misinformation. Larger studies are needed to confirm these effects.

---

## Tools and Setup
- **Hardware:** Boston Dynamics Spot, Bluetooth speaker.  
- **Software:** LLM-based content generator, experiment control scripts (Python / ROS / audio playback tools).  
- **Data Collection:** Self-report questionnaires, observational logs, and behavioral coding.

---

## Repository Structure
```
Credibility-in-Embodied-Agent/
â”‚
â”œâ”€â”€ data/ # Experimental data (anonymized)
â”œâ”€â”€ scripts/ # Code for running and logging experiments
â”œâ”€â”€ analysis/ # Data analysis scripts (Python / R)
â”œâ”€â”€ docs/ # Study design documents, consent forms, visuals
â”œâ”€â”€ results/ # Figures, tables, and summarized findings
â””â”€â”€ README.md # Project overview

```
---

## Ethical Considerations
All procedures follow institutional ethical guidelines for humanâ€“robot interaction studies. Participants are fully debriefed after the experiment to clarify any intentionally introduced hallucinated content.

---

## Course (CS-5761) Instructor 
- Dr. Michael Walker
- Assistant Professor, Computer Science
- Michigan Tech University. 
- [Google Scholar](https://scholar.google.com/citations?user=tM9tcT0AAAAJ&hl=en&oi=sra)

---

## Team 
- [Rosario Curcuru](https://github.com/rosariocurcuru), MSc Student in Computer Science, MTU
- [Ananna Biswas](https://anannabiswas.github.io/), PhD Student in Computational Science & Engineering, MTU
- [Asma Karim](https://github.com/AsmaAbidKarim), MSc Student in Computer Science, MTU
- [Sammi Trost](https://github.com/srtrost), MSc Student in Computer Science, MTU





