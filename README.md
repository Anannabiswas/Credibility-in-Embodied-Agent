
# ðŸ¤– Credibility-in-Embodied-Agent

## Overview
**Credibility-in-Embodied-Agent** is an experimental study designed to explore how **robot embodiment** shape human **trust and belief** in information delivered by AI systems. The project compares interactions with a **physically embodied robot (Boston Dynamics Spot)** and a **voice-only assistant (Bluetooth speaker)**, both delivering identical content generated by a **Large Language Model (LLM)** that includes some intentionally hallucinated statements.

This research aims to uncover when and why humans may **over-trust embodied AI systems**, particularly when false information is presented in socially rich, physically present contexts.

---

## Research Objectives
- **Examine** how robot embodiment (physical presence) affects user trust in AI-delivered content.  
- **Evaluate** whether embodied robots lead to **over-trust** in hallucinated LLM content compared to voice-only assistants.  
- **Develop** design implications for **ethical, transparent embodied AI** that balance trust with accuracy.

---

## Experimental Design
- **Conditions:**  
  - *Embodied Robot:* Boston Dynamics Spot delivers LLM-generated content.  
  - *Voice-Only Assistant:* Bluetooth speaker delivers the same script.  

- **Controls:**  
  - Identical LLM script across all conditions.  
  - Equal interaction time and standardized confederate behavior.  
  - Randomized order and participant blinding to hallucinated content.  
  - Validated post-interaction surveys for trust and credibility assessment.

---

## Hypothesis
We hypothesize that:
- Participants will show **higher trust and belief** in hallucinated content when it is delivered by the **embodied robot**.
- These findings will reveal critical insights into how **embodiment and social context jointly shape trust formation** in AI-human interactions.

---

## Expected Outcomes
- Identification of conditions that foster **appropriate vs. excessive trust** in AI systems.  
- Insights into **context-specific trust** across domains such as **education, healthcare, and industry**.  
- Design guidelines for **trustworthy, transparent embodied AI** systems.

---

## Tools and Setup
- **Hardware:** Boston Dynamics Spot, Bluetooth speaker.  
- **Software:** LLM-based content generator, experiment control scripts (Python / ROS / audio playback tools).  
- **Data Collection:** Self-report questionnaires, observational logs, and behavioral coding.

---

## Repository Structure
Credibility-in-Embodied-Agent/
â”‚
â”œâ”€â”€ data/ # Experimental data (anonymized)
â”œâ”€â”€ scripts/ # Code for running and logging experiments
â”œâ”€â”€ analysis/ # Data analysis scripts (Python / R)
â”œâ”€â”€ docs/ # Study design documents, consent forms, visuals
â”œâ”€â”€ results/ # Figures, tables, and summarized findings
â””â”€â”€ README.md # Project overview


---

## Ethical Considerations
All procedures follow institutional ethical guidelines for humanâ€“robot interaction studies. Participants are fully debriefed after the experiment to clarify any intentionally introduced hallucinated content.

---

## Course (CS-5761) Intructor 
Dr. Michael Walker
Assistant Professor, Computer Science
Michigan Tech University. 
[Google Scholar](https://scholar.google.com/citations?user=tM9tcT0AAAAJ&hl=en&oi=sra)



---

## Team 
[Ananna Biswas](https://anannabiswas.github.io/), PhD Student in Computational Science & Engineering, MTU
[Rosario Curcuru](rosario github link), MSc Student in Computer Science, MTU
[Asma Karim](https://github.com/AsmaAbidKarim), MSc Student in Computer Science, MTU
[Sammi Trost](https://github.com/srtrost), MSc Student in Computer Science, MTU





